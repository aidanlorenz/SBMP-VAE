{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d5af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55de7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff3a8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, device=device):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # encoder - THIS CAN BE MESSED AROUND WITH\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        # decoder - THIS CAN ALSO BE MESSED AROUND WITH\n",
    "        self.point_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "            )\n",
    "        \n",
    "        # CLASS decoder - THIS CAN ALSO BE MESSED AROUND WITH. This is to decode (predict) whether the pt is in collision or not.\n",
    "        self.label_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1), # just decoding 1 thing here: collision or not\n",
    "            nn.Sigmoid() # make the prediction a probability\n",
    "            )\n",
    "     \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    #def decode(self, x):\n",
    "    #    return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        #x_hat = self.decode(z)\n",
    "        x_hat = self.point_decoder(z)\n",
    "        x_hat[2] = sigmoid(x_hat[2])\n",
    "        x_hat[0] = (sigmoid(x_hat[0]) - 0.5)*2*math.pi\n",
    "        x_hat[1] = (sigmoid(x_hat[1]) - 0.5)*2*math.pi\n",
    "        #label_hat = self.label_decoder(z)\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27b0ff30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6010,  0.3273, -0.4814],\n",
       "         [ 0.6849,  0.2926, -0.5046]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.3423,  0.0722],\n",
       "         [-0.3417,  0.0729]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2230, -0.2775],\n",
       "         [-0.2224, -0.2779]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(3,3,2)\n",
    "model(train_data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f5c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_collision_free(x,y):\n",
    "    columns = []\n",
    "    columns.append(((-math.pi/2 - 0.2, -math.pi/2 + 0.2),(-0.1,0.1)))\n",
    "    columns.append(((-0.2, 0.2),(-1.1,-0.9)))\n",
    "    columns.append(((math.pi/2 - 0.2, math.pi/2 + 0.2),(0.9,1.1)))\n",
    "    if x <= -math.pi or x > math.pi or y <= -math.pi or y > math.pi:\n",
    "        raise Exception(\"point needs to be in [-pi,pi] x [-pi,pi]\")\n",
    "    for column in columns:\n",
    "        in_x_col = x >= column[0][0] and x <= column[0][1]\n",
    "        in_y_op = y > column[1][0] and y < column[1][1]\n",
    "        if in_x_col and not in_y_op:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00c1d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isinf(torch.tensor(float('inf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c13c374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var, alpha = 1, beta = 1):\n",
    "    # alpha and beta are parameters to determine how much to weight Euclidean distance and correct classification\n",
    "    \n",
    "    for i in range(len(x_hat)):\n",
    "        if i < len(x_hat) - 1:\n",
    "            if x_hat[i] > math.pi or x_hat[i] <= -math.pi:\n",
    "                return 1e6\n",
    "        else:\n",
    "            if x_hat[i] < 0 or x_hat[i] > 1:\n",
    "                return 1e6\n",
    "    \n",
    "    # Euclidean reconstruction loss\n",
    "    dist = torch.sqrt(torch.sum((x_hat - x)**2)) # Euclidean distance\n",
    "    reconstruction_loss = alpha*dist\n",
    "    \n",
    "    # BCE label loss\n",
    "    #label = is_collision_free(x) # or label(x)\n",
    "    #pred = COLLISIONCHECKER(x_hat)\n",
    "    \n",
    "    # need probabilities that it is collision and not collision. So we need a collision predictor\n",
    "    # IDEA: make the COLLISIONCHECKER function a neural network classifier so that we can backprop.\n",
    "    \n",
    "    #label_loss = beta*nn.functional.binary_cross_entropy(label, label_hat, reduction='sum')\n",
    "    \n",
    "    # KL divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    #return reconstruction_loss + label_loss + KLD\n",
    "    return torch.tensor(reconstruction_loss + KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba2a400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_collision_free(2.5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cb5bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcb969ed300>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples = 500\n",
    "\n",
    "x_values = [ math.pi * (n/num_samples) - math.pi * (1-n/num_samples) for n in range(1,num_samples+1)]\n",
    "y_values = x_values\n",
    "value_grid = []\n",
    "for y in y_values:\n",
    "    y_section = [(x,y,is_collision_free(x,y)) for x in x_values]\n",
    "    value_grid.append(y_section)\n",
    "\n",
    "x_collisions = []\n",
    "y_collisions = []\n",
    "for y_row in value_grid:\n",
    "    for entry in y_row:\n",
    "        if not entry[2]:\n",
    "            x_collisions.append(entry[0])\n",
    "            y_collisions.append(entry[1])\n",
    "plt.scatter(x_collisions, y_collisions, s =5)\n",
    "plt.scatter([-3.1415,-3.1415, 3.1415,3.1415], [-3.1415,3.1415,-3.1415,3.1415], s = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905b4ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(value_grid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aae9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [value_grid[i][j] for i in range(500) for j in range(500)]\n",
    "train_data = torch.tensor(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f83dc510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250000, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82bb9520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, tensor([-3.1290, -3.1290,  1.0000]))\n"
     ]
    }
   ],
   "source": [
    "for elt in enumerate(train_data):\n",
    "    print(elt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0834f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4440a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27ab538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[ 2.1237,  2.7520,  1.0000],\n",
      "        [-0.5278,  0.6786,  1.0000],\n",
      "        [-1.3195,  2.3876,  1.0000],\n",
      "        ...,\n",
      "        [-1.5834, -0.5655,  0.0000],\n",
      "        [ 2.0358, -1.7844,  1.0000],\n",
      "        [ 1.6211,  2.9782,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(batch_idx, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bfaab90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss_float = 0.\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            #x = x.view(batch_size, x_dim).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_output = model(batch)\n",
    "\n",
    "            x_hats, means, log_vars = batch_output[0], batch_output[1], batch_output[2] \n",
    "            loss = torch.tensor(0.)\n",
    "            for i in range(batch_size):\n",
    "                x_hat, mean, log_var = x_hats[i], means[i], log_vars[i]\n",
    "                loss += loss_function(batch[i], x_hat, mean, log_var)\n",
    "                #print(type(loss))\n",
    "            \n",
    "            overall_loss_float += loss\n",
    "            \n",
    "        overall_loss = torch.tensor(overall_loss_float, requires_grad = True)\n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cdc52cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0.).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7c2c1a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[89], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#x = x.view(batch_size, x_dim).to(device)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x_hats, means, log_vars \u001b[38;5;241m=\u001b[39m batch_output[\u001b[38;5;241m0\u001b[39m], batch_output[\u001b[38;5;241m1\u001b[39m], batch_output[\u001b[38;5;241m2\u001b[39m] \n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 54\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#x_hat = self.decode(z)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_decoder(z)\n\u001b[0;32m---> 54\u001b[0m x_hat[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m x_hat[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (sigmoid(x_hat[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi\n\u001b[1;32m     56\u001b[0m x_hat[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (sigmoid(x_hat[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi\n",
      "Cell \u001b[0;32mIn[100], line 4\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "model = VAE(3,3,2)\n",
    "optimizer = Adam(model.parameters(), lr=1e-1)\n",
    "train(model, optimizer, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a479f882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.6515, -2.8777,  1.0000]),\n",
       " (tensor([0.1925, 0.6315, 0.1759], grad_fn=<ViewBackward0>),\n",
       "  tensor([ 0.0796, -0.4391], grad_fn=<ViewBackward0>),\n",
       "  tensor([-0.1793, -0.1845], grad_fn=<ViewBackward0>)))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_digit(mean, var):\n",
    "    z_sample = torch.tensor([[mean, var]], dtype=torch.float).to(device)\n",
    "    x_decoded = model.decode(z_sample)\n",
    "    digit = x_decoded.detach().cpu().reshape(28, 28) # reshape vector to 2d array\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_digit(0.0, 1.0), generate_digit(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f7030",
   "metadata": {},
   "source": [
    "# Attempt with Autoencoder (non-variational)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657f033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
